{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh1XJxNiet0i2Od0zsJw8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psingh-xyz/OSDA_course/blob/Autumn_2022/ProjectWork_PriyankaSingh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEUSKX-w8XU",
        "outputId": "0e0df243-f089-4610-bbdf-5b409616abbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.11.17)\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.6.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia-api\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRRZbbJyxfIc",
        "outputId": "730ba71b-c705-4604-844b-199eae0a1ef4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.11.17)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=eb34811bfeb11683a1e970154f5be04281c34adcbf19e0da2e385045185d9626\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data for stemming\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amz9sqs9yK_F",
        "outputId": "bf15398f-8319-46e4-a048-d9c376aea847"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "\n",
        "def get_wikipedia_text(title, language='en'):\n",
        "    try:\n",
        "        wikipedia.set_lang(language)\n",
        "        page = wikipedia.page(title)\n",
        "        return page.content\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Error: Wikipedia page '{title}' not found.\")\n",
        "        return None\n",
        "\n",
        "def stem_and_count_frequency(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize words using regex\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    word_frequency = Counter(stemmed_words)\n",
        "    return word_frequency\n",
        "\n",
        "def main():\n",
        "    wikipedia_title = \"Scientific text\"\n",
        "    wiki_text = get_wikipedia_text(wikipedia_title)\n",
        "\n",
        "    if wiki_text:\n",
        "        word_frequency = stem_and_count_frequency(wiki_text)\n",
        "\n",
        "        print(\"Top 100 stemmed words and their frequencies:\")\n",
        "        for word, freq in word_frequency.most_common(100):\n",
        "            print(f\"{word}: {freq}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb5WbDi0xCNI",
        "outputId": "c96ba3fb-9e71-48c0-c0b5-fcf74a6ceb6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 100 stemmed words and their frequencies:\n",
            "the: 731\n",
            "of: 483\n",
            "and: 328\n",
            "to: 301\n",
            "a: 270\n",
            "in: 241\n",
            "is: 173\n",
            "that: 164\n",
            "scientif: 145\n",
            "be: 133\n",
            "as: 123\n",
            "or: 120\n",
            "method: 112\n",
            "it: 103\n",
            "for: 99\n",
            "by: 96\n",
            "scienc: 89\n",
            "from: 74\n",
            "are: 70\n",
            "hypothesi: 70\n",
            "can: 69\n",
            "s: 67\n",
            "on: 61\n",
            "experi: 60\n",
            "which: 59\n",
            "with: 57\n",
            "observ: 56\n",
            "an: 51\n",
            "such: 49\n",
            "not: 49\n",
            "research: 49\n",
            "test: 48\n",
            "their: 47\n",
            "result: 47\n",
            "predict: 46\n",
            "use: 46\n",
            "theori: 45\n",
            "other: 45\n",
            "thi: 44\n",
            "have: 39\n",
            "but: 38\n",
            "wa: 36\n",
            "scientist: 35\n",
            "at: 34\n",
            "if: 33\n",
            "might: 31\n",
            "may: 31\n",
            "dna: 31\n",
            "ha: 29\n",
            "hypothes: 29\n",
            "when: 29\n",
            "exampl: 29\n",
            "studi: 28\n",
            "new: 28\n",
            "experiment: 27\n",
            "more: 27\n",
            "inquiri: 26\n",
            "gener: 26\n",
            "practic: 26\n",
            "mathemat: 26\n",
            "what: 25\n",
            "explan: 25\n",
            "data: 24\n",
            "also: 24\n",
            "some: 24\n",
            "these: 23\n",
            "they: 23\n",
            "reason: 23\n",
            "measur: 22\n",
            "process: 22\n",
            "then: 22\n",
            "question: 22\n",
            "been: 22\n",
            "truth: 22\n",
            "work: 22\n",
            "evid: 22\n",
            "model: 21\n",
            "structur: 21\n",
            "knowledg: 20\n",
            "one: 20\n",
            "time: 20\n",
            "form: 20\n",
            "differ: 20\n",
            "would: 20\n",
            "how: 19\n",
            "induct: 19\n",
            "base: 19\n",
            "field: 19\n",
            "possibl: 19\n",
            "often: 19\n",
            "hi: 19\n",
            "see: 18\n",
            "definit: 18\n",
            "ani: 18\n",
            "will: 18\n",
            "he: 17\n",
            "element: 17\n",
            "idea: 17\n",
            "involv: 16\n",
            "make: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2xzA1hAJlQ",
        "outputId": "a3178013-f099-48c5-ea93-e2a520c64291"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "\n",
        "def pdf_to_text(pdf_path, text_path):\n",
        "        with open(pdf_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "\n",
        "            # Create the directory if it doesn't exist\n",
        "            os.makedirs(os.path.dirname(text_path), exist_ok=True)\n",
        "\n",
        "            with open(text_path, 'w', encoding='utf-8') as text_file:\n",
        "                for page_num in range(num_pages):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text_file.write(page.extract_text())\n",
        "\n",
        "        print(f\"Conversion successful. Text saved to: {text_path}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "pdf_file_path = '/content/ED611000.pdf'\n",
        "text_file_path = '/content/drive/MyDrive/output.txt'\n",
        "\n",
        "pdf_to_text(pdf_file_path, text_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrXdMKBmDGyH",
        "outputId": "e46cdf1c-b6a6-444f-c180-2d96b7a19f46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion successful. Text saved to: /content/drive/MyDrive/output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def count_word_frequencies(text_path):\n",
        "    with open(text_path, 'r', encoding='utf-8') as text_file:\n",
        "        text = text_file.read()\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Count the frequencies of each word\n",
        "    word_frequencies = Counter(words)\n",
        "\n",
        "    return word_frequencies\n",
        "\n",
        "word_frequencies = count_word_frequencies(text_file_path)\n",
        "\n",
        "# Print the word frequencies\n",
        "for word, frequency in word_frequencies.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtfkA97BC2aT",
        "outputId": "bbb422ae-0e6d-4c37-92d3-c23cd14b0d07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glossary: 3\n",
            "of: 986\n",
            "key: 10\n",
            "terms: 8\n",
            "in: 465\n",
            "educational: 9\n",
            "research: 209\n",
            "by: 118\n",
            "abdullah: 1\n",
            "noori: 1\n",
            "assistant: 1\n",
            "professor: 1\n",
            "department: 1\n",
            "english: 1\n",
            "kabul: 1\n",
            "university: 2\n",
            "orcid: 1\n",
            "0000: 1\n",
            "0003: 1\n",
            "2141: 1\n",
            "3675: 1\n",
            "email: 1\n",
            "abdullahm40gmailcom: 1\n",
            "final: 1\n",
            "copy: 1\n",
            "date: 2\n",
            "completion: 2\n",
            "februar: 1\n",
            "y: 4\n",
            "3: 2\n",
            "2021: 1\n",
            "te: 2\n",
            "rms: 1\n",
            "the: 1390\n",
            "purpose: 5\n",
            "this: 48\n",
            "is: 319\n",
            "to: 605\n",
            "help: 3\n",
            "novice: 1\n",
            "researchers: 33\n",
            "understanding: 7\n",
            "basic: 6\n",
            "terminologies: 1\n",
            "it: 81\n",
            "provides: 8\n",
            "definitions: 2\n",
            "many: 12\n",
            "used: 99\n",
            "guide: 1\n",
            "books: 3\n",
            "conducting: 6\n",
            "qualitative: 46\n",
            "quantitative: 14\n",
            "and: 386\n",
            "mixed: 7\n",
            "methods: 26\n",
            "are: 223\n",
            "arranged: 1\n",
            "alphabetical: 1\n",
            "order: 29\n",
            "❖: 800\n",
            "abstract: 1\n",
            "a: 1104\n",
            "brief: 2\n",
            "summary: 6\n",
            "project: 13\n",
            "its: 15\n",
            "findings: 19\n",
            "study: 141\n",
            "that: 387\n",
            "describes: 7\n",
            "most: 17\n",
            "important: 9\n",
            "aspe: 1\n",
            "cts: 1\n",
            "including: 10\n",
            "major: 3\n",
            "results: 52\n",
            "conclusions: 11\n",
            "accessible: 4\n",
            "population: 90\n",
            "from: 126\n",
            "which: 186\n",
            "researcher: 78\n",
            "can: 68\n",
            "realistically: 1\n",
            "select: 4\n",
            "subjects: 32\n",
            "for: 146\n",
            "sample: 110\n",
            "entitled: 1\n",
            "generalize: 6\n",
            "acculturation: 1\n",
            "refers: 11\n",
            "proc: 1\n",
            "ess: 1\n",
            "adapting: 1\n",
            "another: 19\n",
            "culture: 4\n",
            "particularly: 1\n",
            "reference: 4\n",
            "blending: 2\n",
            "with: 93\n",
            "majority: 2\n",
            "eg: 26\n",
            "an: 178\n",
            "immigrant: 1\n",
            "adopting: 2\n",
            "american: 1\n",
            "customs: 2\n",
            "achievement: 2\n",
            "test: 50\n",
            "instrument: 33\n",
            "measure: 41\n",
            "proficiency: 1\n",
            "level: 19\n",
            "individuals: 54\n",
            "given: 21\n",
            "areas: 3\n",
            "o: 13\n",
            "f: 10\n",
            "knowledge: 18\n",
            "or: 395\n",
            "skill: 4\n",
            "action: 11\n",
            "plan: 8\n",
            "implement: 1\n",
            "change: 17\n",
            "as: 106\n",
            "result: 16\n",
            "type: 26\n",
            "focused: 2\n",
            "on: 137\n",
            "specific: 45\n",
            "local: 3\n",
            "problem: 15\n",
            "resulting: 4\n",
            "address: 4\n",
            "adjusted: 4\n",
            "r: 19\n",
            "squared: 4\n",
            "how: 29\n",
            "well: 12\n",
            "independent: 61\n",
            "predictor: 20\n",
            "variables: 115\n",
            "predict: 7\n",
            "dependent: 42\n",
            "outcome: 27\n",
            "variable: 180\n",
            "administrative: 1\n",
            "data: 177\n",
            "information: 50\n",
            "about: 39\n",
            "individual: 20\n",
            "children: 5\n",
            "families: 3\n",
            "andor: 12\n",
            "providers: 1\n",
            "early: 2\n",
            "care: 4\n",
            "education: 4\n",
            "other: 44\n",
            "family: 2\n",
            "benefits: 3\n",
            "re: 5\n",
            "collected: 19\n",
            "maintained: 1\n",
            "part: 14\n",
            "operation: 3\n",
            "government: 1\n",
            "programs: 4\n",
            "adult: 2\n",
            "supervisor: 1\n",
            "who: 29\n",
            "oversees: 1\n",
            "student: 3\n",
            "experiment: 20\n",
            "person: 7\n",
            "should: 11\n",
            "be: 144\n",
            "familiar: 2\n",
            "student’s: 3\n",
            "area: 9\n",
            "affective: 1\n",
            "measures: 26\n",
            "procedures: 12\n",
            "devices: 1\n",
            "obtain: 5\n",
            "quantified: 1\n",
            "descriptions: 2\n",
            "feelings: 3\n",
            "emotional: 1\n",
            "states: 9\n",
            "dispositions: 1\n",
            "ageequivalent: 1\n",
            "score: 47\n",
            "indicates: 13\n",
            "age: 5\n",
            "particular: 35\n",
            "performance: 18\n",
            "typical: 12\n",
            "aggregate: 2\n",
            "total: 3\n",
            "created: 3\n",
            "smaller: 7\n",
            "units: 12\n",
            "instance: 5\n",
            "county: 3\n",
            "populations: 5\n",
            "cities: 2\n",
            "rural: 1\n",
            "etc: 9\n",
            "comprise: 1\n",
            "verb: 1\n",
            "into: 28\n",
            "large: 7\n",
            "unit: 8\n",
            "alpha: 1\n",
            "probability: 32\n",
            "statistical: 68\n",
            "will: 26\n",
            "find: 9\n",
            "significant: 10\n",
            "differences: 11\n",
            "between: 83\n",
            "groups: 61\n",
            "predictors: 2\n",
            "when: 52\n",
            "fact: 5\n",
            "there: 22\n",
            "none: 2\n",
            "alternating: 1\n",
            "treatment: 34\n",
            "design: 51\n",
            "single: 10\n",
            "subject: 16\n",
            "studying: 4\n",
            "two: 69\n",
            "more: 58\n",
            "treatments: 4\n",
            "alternative: 9\n",
            "hypothesis: 37\n",
            "experimental: 46\n",
            "stating: 3\n",
            "some: 32\n",
            "real: 3\n",
            "difference: 22\n",
            "null: 13\n",
            "no: 19\n",
            "anecdotal: 1\n",
            "record: 5\n",
            "s: 15\n",
            "records: 3\n",
            "observed: 25\n",
            "behaviors: 17\n",
            "written: 7\n",
            "down: 5\n",
            "form: 27\n",
            "anecdotes: 2\n",
            "best: 7\n",
            "tell: 1\n",
            "exactly: 2\n",
            "what: 30\n",
            "participant: 17\n",
            "did: 3\n",
            "said: 6\n",
            "without: 11\n",
            "making: 6\n",
            "evaluative: 2\n",
            "statements: 6\n",
            "process: 38\n",
            "reporting: 3\n",
            "anonymity: 2\n",
            "condition: 6\n",
            "one: 82\n",
            "knows: 3\n",
            "identities: 2\n",
            "participants: 22\n",
            "identity: 2\n",
            "remains: 4\n",
            "unknown: 5\n",
            "not: 79\n",
            "linked: 1\n",
            "provided: 2\n",
            "appetency: 1\n",
            "clear: 3\n",
            "understandabl: 1\n",
            "e: 21\n",
            "representation: 2\n",
            "aptitude: 2\n",
            "future: 7\n",
            "situation: 14\n",
            "association: 5\n",
            "relationship: 49\n",
            "objects: 13\n",
            "associational: 1\n",
            "general: 17\n",
            "looks: 2\n",
            "relationships: 15\n",
            "having: 12\n",
            "predictive: 5\n",
            "explanatory: 4\n",
            "power: 4\n",
            "both: 17\n",
            "correctional: 1\n",
            "causal: 15\n",
            "comparative: 4\n",
            "studies: 16\n",
            "examples: 2\n",
            "assumption: 5\n",
            "any: 27\n",
            "assertion: 2\n",
            "presumed: 1\n",
            "true: 9\n",
            "but: 33\n",
            "actually: 9\n",
            "veri: 1\n",
            "fied: 1\n",
            "assumptions: 6\n",
            "described: 3\n",
            "first: 10\n",
            "sections: 1\n",
            "proposal: 2\n",
            "report: 7\n",
            "attitude: 2\n",
            "scale: 27\n",
            "set: 23\n",
            "responds: 2\n",
            "attrition: 2\n",
            "rate: 9\n",
            "at: 63\n",
            "drop: 2\n",
            "out: 18\n",
            "longitudinal: 5\n",
            "if: 31\n",
            "types: 9\n",
            "faster: 1\n",
            "than: 36\n",
            "introduce: 1\n",
            "bias: 26\n",
            "threaten: 1\n",
            "internal: 19\n",
            "validity: 43\n",
            "loss: 2\n",
            "members: 13\n",
            "over: 20\n",
            "time: 48\n",
            "follow: 8\n",
            "up: 8\n",
            "tests: 11\n",
            "audit: 3\n",
            "trail: 1\n",
            "systematic: 20\n",
            "presentation: 6\n",
            "material: 2\n",
            "gathered: 7\n",
            "within: 25\n",
            "naturalistic: 8\n",
            "allows: 6\n",
            "others: 2\n",
            "researcher’s: 2\n",
            "thinking: 1\n",
            "autonomy: 1\n",
            "capacity: 1\n",
            "think: 2\n",
            "deci: 1\n",
            "de: 1\n",
            "act: 5\n",
            "basis: 4\n",
            "such: 36\n",
            "thought: 3\n",
            "decision: 5\n",
            "freely: 3\n",
            "independently: 2\n",
            "let: 1\n",
            "hindrance: 1\n",
            "average: 16\n",
            "value: 36\n",
            "mean: 34\n",
            "median: 8\n",
            "mode: 7\n",
            "representing: 4\n",
            "normal: 18\n",
            "middle: 3\n",
            "thre: 1\n",
            "main: 4\n",
            "numerical: 6\n",
            "these: 17\n",
            "axiom: 1\n",
            "statement: 19\n",
            "widely: 2\n",
            "accepted: 3\n",
            "truth: 1\n",
            "background: 4\n",
            "question: 28\n",
            "asked: 6\n",
            "interviewer: 7\n",
            "questionnaire: 5\n",
            "respondents: 23\n",
            "occupation: 1\n",
            "bar: 2\n",
            "graph: 6\n",
            "graphic: 3\n",
            "way: 27\n",
            "illustrating: 1\n",
            "among: 14\n",
            "baseline: 3\n",
            "control: 35\n",
            "measurement: 16\n",
            "carried: 5\n",
            "before: 7\n",
            "behaviorism: 3\n",
            "school: 7\n",
            "psychological: 1\n",
            "concerned: 6\n",
            "wit: 1\n",
            "h: 7\n",
            "observable: 5\n",
            "tangible: 1\n",
            "objective: 5\n",
            "facts: 2\n",
            "behavior: 22\n",
            "rather: 9\n",
            "subjective: 1\n",
            "phenomena: 5\n",
            "thoughts: 3\n",
            "emotions: 1\n",
            "impulses: 1\n",
            "contemporary: 1\n",
            "also: 35\n",
            "emphasizes: 1\n",
            "mental: 3\n",
            "fantasies: 1\n",
            "extent: 19\n",
            "th: 8\n",
            "ey: 1\n",
            "directly: 3\n",
            "measured: 18\n",
            "beliefs: 7\n",
            "ideas: 9\n",
            "doctrines: 2\n",
            "tenets: 1\n",
            "grounds: 2\n",
            "immediately: 1\n",
            "susceptible: 1\n",
            "rigorous: 3\n",
            "proof: 1\n",
            "bell: 6\n",
            "curve: 8\n",
            "frequency: 14\n",
            "distribution: 60\n",
            "statistics: 15\n",
            "shaped: 4\n",
            "like: 5\n",
            "bellshaped: 1\n",
            "characteristic: 14\n",
            "symmetrical: 3\n",
            "extends: 1\n",
            "infinitely: 1\n",
            "directions: 1\n",
            "under: 15\n",
            "curve10: 1\n",
            "benchmarking: 1\n",
            "systematically: 14\n",
            "measuring: 6\n",
            "comparing: 3\n",
            "operations: 2\n",
            "nd: 2\n",
            "outcomes: 8\n",
            "organizations: 2\n",
            "systems: 7\n",
            "processes: 7\n",
            "against: 3\n",
            "agreed: 1\n",
            "upon: 2\n",
            "inclass: 1\n",
            "frames: 1\n",
            "beta: 3\n",
            "error: 43\n",
            "chan: 1\n",
            "ce: 1\n",
            "variations: 3\n",
            "reality: 11\n",
            "manipulation: 6\n",
            "intervention: 26\n",
            "group: 94\n",
            "variance: 14\n",
            "means: 11\n",
            "various: 5\n",
            "different: 36\n",
            "each: 42\n",
            "balance: 2\n",
            "accuracy: 2\n",
            "use: 19\n",
            "appear: 4\n",
            "via: 1\n",
            "sampling: 64\n",
            "frame: 7\n",
            "random: 47\n",
            "non: 18\n",
            "response: 17\n",
            "occur: 9\n",
            "stages: 1\n",
            "while: 2\n",
            "interviewing: 4\n",
            "questions: 42\n",
            "analyzed: 3\n",
            "presented: 4\n",
            "representative: 12\n",
            "generalizable: 4\n",
            "wider: 3\n",
            "bibliography: 2\n",
            "list: 9\n",
            "referred: 16\n",
            "usually: 15\n",
            "appears: 4\n",
            "end: 5\n",
            "separate: 4\n",
            "section: 1\n",
            "known: 6\n",
            "appendix: 1\n",
            "bimodal: 1\n",
            "scores: 47\n",
            "frequently: 4\n",
            "occurring: 2\n",
            "biographybiographica: 1\n",
            "l: 7\n",
            "works: 4\n",
            "clarify: 3\n",
            "life: 6\n",
            "experiences: 6\n",
            "bootstrapping: 1\n",
            "popular: 1\n",
            "method: 51\n",
            "estimation: 5\n",
            "surveys: 8\n",
            "consists: 6\n",
            "subsampling: 1\n",
            "initial: 2\n",
            "stratum: 1\n",
            "simple: 5\n",
            "subsample: 3\n",
            "selected: 27\n",
            "replacement: 1\n",
            "bracketing: 1\n",
            "working: 3\n",
            "husserlian: 1\n",
            "phenomenological: 1\n",
            "tradition: 1\n",
            "identify: 10\n",
            "their: 33\n",
            "preconceived: 1\n",
            "opinions: 4\n",
            "phenomenon: 12\n",
            "investigation: 11\n",
            "personal: 3\n",
            "biases: 4\n",
            "experience: 6\n",
            "might: 4\n",
            "influence: 10\n",
            "seen: 3\n",
            "heard: 3\n",
            "reported: 2\n",
            "case: 14\n",
            "collection: 16\n",
            "detailed: 6\n",
            "small: 10\n",
            "inc: 1\n",
            "luding: 1\n",
            "derived: 10\n",
            "themselves: 2\n",
            "intensive: 2\n",
            "current: 3\n",
            "past: 4\n",
            "organization: 2\n",
            "informati: 1\n",
            "accounts: 3\n",
            "categorical: 17\n",
            "discrete: 5\n",
            "numeric: 4\n",
            "categories: 25\n",
            "gender: 12\n",
            "marital: 2\n",
            "status: 4\n",
            "numeri: 1\n",
            "cal: 1\n",
            "codes: 4\n",
            "they: 34\n",
            "cannot: 8\n",
            "ranked: 4\n",
            "added: 2\n",
            "multiplied: 2\n",
            "nominal: 4\n",
            "differ: 9\n",
            "only: 18\n",
            "kind: 1\n",
            "amount: 8\n",
            "degree: 23\n",
            "analysis: 80\n",
            "seeks: 5\n",
            "esta: 1\n",
            "blish: 1\n",
            "cause: 10\n",
            "effect: 43\n",
            "explanation: 13\n",
            "attempt: 5\n",
            "explain: 4\n",
            "occurrence: 2\n",
            "event: 8\n",
            "identifying: 2\n",
            "hypothesizing: 1\n",
            "affects: 6\n",
            "model: 14\n",
            "represents: 2\n",
            "established: 2\n",
            "shows: 2\n",
            "nothing: 1\n",
            "else: 3\n",
            "causes: 3\n",
            "i: 3\n",
            "n: 16\n",
            "establishes: 1\n",
            "much: 2\n",
            "shown: 6\n",
            "determine: 22\n",
            "consequences: 2\n",
            "existing: 6\n",
            "ex: 2\n",
            "post: 5\n",
            "facto: 1\n",
            "census: 3\n",
            "acquire: 1\n",
            "every: 10\n",
            "member: 9\n",
            "causality: 1\n",
            "relation: 2\n",
            "ceiling: 2\n",
            "highest: 3\n",
            "limit: 6\n",
            "assessed: 5\n",
            "dividuals: 1\n",
            "perform: 4\n",
            "near: 2\n",
            "above: 2\n",
            "upper: 3\n",
            "have: 29\n",
            "reached: 2\n",
            "assessment: 7\n",
            "may: 37\n",
            "providing: 3\n",
            "valid: 8\n",
            "estimate: 13\n",
            "levels: 7\n",
            "all: 34\n",
            "instead: 2\n",
            "target: 12\n",
            "central: 13\n",
            "theorem: 3\n",
            "mathematical: 6\n",
            "observations: 19\n",
            "finite: 3\n",
            "justification: 3\n",
            "widespread: 1\n",
            "analyses: 9\n",
            "based: 31\n",
            "tendency: 12\n",
            "describing: 4\n",
            "characterizing: 1\n",
            "common: 12\n",
            "values: 36\n",
            "dist: 2\n",
            "ribution: 1\n",
            "include: 9\n",
            "chaos: 1\n",
            "theory: 32\n",
            "methodology: 9\n",
            "science: 7\n",
            "empha: 1\n",
            "sizes: 3\n",
            "rarity: 1\n",
            "laws: 2\n",
            "need: 3\n",
            "very: 2\n",
            "bases: 1\n",
            "importance: 2\n",
            "exceptions: 1\n",
            "overall: 3\n",
            "patterns: 13\n",
            "chi: 1\n",
            "square: 5\n",
            "statistic: 15\n",
            "testing: 10\n",
            "associations: 1\n",
            "goodness: 1\n",
            "offit: 1\n",
            "whether: 16\n",
            "come: 1\n",
            "chisquare: 2\n",
            "parametric: 4\n",
            "compares: 2\n",
            "expected: 6\n",
            "proportion: 8\n",
            "ratio: 9\n",
            "actual: 7\n",
            "significance: 11\n",
            "appropriate: 6\n",
            "counts: 2\n",
            "frequencies: 2\n",
            "see: 3\n",
            "significantly: 2\n",
            "ent: 3\n",
            "citation: 3\n",
            "acknowledging: 2\n",
            "documenting: 1\n",
            "source: 5\n",
            "preparing: 1\n",
            "assignment: 5\n",
            "documentation: 2\n",
            "full: 4\n",
            "lists: 2\n",
            "accurate: 5\n",
            "author: 2\n",
            "title: 1\n",
            "publication: 3\n",
            "rela: 1\n",
            "ted: 4\n",
            "number: 20\n",
            "styles: 1\n",
            "claim: 1\n",
            "similar: 8\n",
            "made: 8\n",
            "affirmed: 1\n",
            "evidence: 9\n",
            "classification: 1\n",
            "ordering: 1\n",
            "related: 12\n",
            "phenome: 1\n",
            "na: 1\n",
            "according: 6\n",
            "characteristics: 35\n",
            "attributes: 3\n",
            "closed: 3\n",
            "followed: 1\n",
            "predetermined: 7\n",
            "choices: 1\n",
            "respondent’s: 1\n",
            "reply: 1\n",
            "placed: 2\n",
            "ended: 8\n",
            "al: 2\n",
            "ternative: 1\n",
            "responses: 7\n",
            "respondent: 15\n",
            "selects: 3\n",
            "item: 8\n",
            "cluster: 6\n",
            "where: 20\n",
            "share: 3\n",
            "trait: 1\n",
            "grouped: 1\n",
            "together: 3\n",
            "da: 2\n",
            "ta: 2\n",
            "collector: 1\n",
            "certain: 10\n",
            "geographically: 3\n",
            "disperse: 1\n",
            "clusters: 7\n",
            "potential: 2\n",
            "randomly: 12\n",
            "then: 16\n",
            "respond: 7\n",
            "ents: 2\n",
            "pre: 2\n",
            "identified: 6\n",
            "samplingcluster: 1\n",
            "selection: 19\n",
            "called: 8\n",
            "included: 6\n",
            "preferably: 2\n",
            "larger: 14\n",
            "codebook: 2\n",
            "structure: 3\n",
            "content: 11\n",
            "layout: 1\n",
            "typically: 5\n",
            "gives: 4\n",
            "names: 1\n",
            "assigned: 6\n",
            "facilitate: 1\n",
            "coding: 14\n",
            "procedure: 15\n",
            "transforming: 1\n",
            "raw: 6\n",
            "standardized: 6\n",
            "format: 1\n",
            "purposes: 3\n",
            "involves: 10\n",
            "recurrent: 1\n",
            "words: 7\n",
            "concepts: 13\n",
            "themes: 1\n",
            "positivist: 3\n",
            "attaching: 1\n",
            "cohort: 5\n",
            "people: 25\n",
            "sharing: 1\n",
            "demographic: 3\n",
            "through: 17\n",
            "example: 20\n",
            "born: 2\n",
            "same: 35\n",
            "year: 4\n",
            "constitute: 2\n",
            "birth: 1\n",
            "analytic: 1\n",
            "factor: 4\n",
            "grou: 1\n",
            "p: 10\n",
            "entering: 1\n",
            "college: 1\n",
            "survey: 29\n",
            "studied: 7\n",
            "taking: 7\n",
            "d: 12\n",
            "ifferent: 2\n",
            "samples: 23\n",
            "points: 20\n",
            "conceptually: 3\n",
            "collaboration: 2\n",
            "service: 4\n",
            "users: 4\n",
            "careers: 2\n",
            "active: 2\n",
            "partners: 1\n",
            "responsibilities: 1\n",
            "opinio: 1\n",
            "ns: 2\n",
            "equal: 10\n",
            "weight: 4\n",
            "those: 16\n",
            "professionals: 1\n",
            "stage: 1\n",
            "collective: 1\n",
            "multiple: 12\n",
            "cases: 6\n",
            "comparability: 1\n",
            "quality: 7\n",
            "evaluated: 2\n",
            "similarity: 5\n",
            "comparison: 8\n",
            "receives: 7\n",
            "qualified: 2\n",
            "complete: 4\n",
            "interview: 23\n",
            "computer: 6\n",
            "search: 7\n",
            "literature: 10\n",
            "whereby: 10\n",
            "locate: 2\n",
            "topic: 13\n",
            "support: 5\n",
            "contrad: 1\n",
            "ict: 1\n",
            "concurrent: 1\n",
            "administered: 2\n",
            "criterion: 12\n",
            "available: 3\n",
            "confidentiality: 2\n",
            "except: 6\n",
            "has: 29\n",
            "disclosed: 1\n",
            "trust: 2\n",
            "expectatio: 1\n",
            "revealed: 1\n",
            "ways: 7\n",
            "violate: 1\n",
            "original: 6\n",
            "consent: 5\n",
            "agreement: 8\n",
            "unless: 1\n",
            "permission: 1\n",
            "granted: 1\n",
            "protection: 1\n",
            "human: 6\n",
            "disclosure: 2\n",
            "confirming: 1\n",
            "validate: 2\n",
            "extend: 1\n",
            "previous: 2\n",
            "conformability: 1\n",
            "objectivity: 2\n",
            "could: 6\n",
            "confirmed: 1\n",
            "confounding: 2\n",
            "variab: 1\n",
            "le: 4\n",
            "interest: 12\n",
            "distorts: 1\n",
            "does: 11\n",
            "unforeseen: 1\n",
            "unaccounted: 1\n",
            "jeopardizes: 1\n",
            "reliability: 17\n",
            "experiments: 5\n",
            "patient: 1\n",
            "agrees: 1\n",
            "coercion: 2\n",
            "pressure: 1\n",
            "involved: 5\n",
            "consistency: 7\n",
            "answered: 1\n",
            "similarly: 1\n",
            "constant: 7\n",
            "stays: 1\n",
            "during: 20\n",
            "grounded: 6\n",
            "newly: 1\n",
            "continually: 1\n",
            "compared: 8\n",
            "previously: 1\n",
            "refine: 1\n",
            "development: 2\n",
            "theoretical: 12\n",
            "constitutive: 1\n",
            "definition: 2\n",
            "meaning: 6\n",
            "term: 13\n",
            "using: 19\n",
            "describe: 10\n",
            "meant: 2\n",
            "construct: 6\n",
            "concept: 10\n",
            "creation: 2\n",
            "device: 3\n",
            "observation: 33\n",
            "constructivism: 1\n",
            "idea: 4\n",
            "reali: 1\n",
            "ty: 1\n",
            "socially: 1\n",
            "constructed: 3\n",
            "view: 8\n",
            "understood: 2\n",
            "outside: 5\n",
            "humans: 1\n",
            "interact: 1\n",
            "discovered: 1\n",
            "constructivists: 1\n",
            "believe: 2\n",
            "learning: 3\n",
            "self: 1\n",
            "directed: 1\n",
            "eit: 1\n",
            "her: 7\n",
            "cognitive: 2\n",
            "would: 22\n",
            "postulate: 1\n",
            "consultation: 1\n",
            "carers: 4\n",
            "views: 6\n",
            "taken: 8\n",
            "account: 13\n",
            "necessarily: 1\n",
            "consultants: 1\n",
            "hav: 1\n",
            "description: 12\n",
            "manifest: 2\n",
            "latent: 2\n",
            "print: 2\n",
            "communications: 1\n",
            "t: 25\n",
            "he: 8\n",
            "obtained: 20\n",
            "documents: 2\n",
            "filed: 1\n",
            "notes: 10\n",
            "face: 5\n",
            "deliberately: 1\n",
            "targets: 1\n",
            "acknowledged: 1\n",
            "experts: 2\n",
            "give: 6\n",
            "matches: 1\n",
            "requirements: 2\n",
            "context: 6\n",
            "effects: 15\n",
            "resulted: 1\n",
            "environment: 4\n",
            "external: 7\n",
            "itself: 6\n",
            "sensitivity: 2\n",
            "awareness: 1\n",
            "factors: 9\n",
            "cultural: 4\n",
            "contextualization: 1\n",
            "placing: 1\n",
            "informationdata: 1\n",
            "perspective: 8\n",
            "especially: 7\n",
            "ethnography: 8\n",
            "contingency: 2\n",
            "coefficient: 10\n",
            "index: 9\n",
            "cross: 6\n",
            "break: 1\n",
            "table: 11\n",
            "whose: 6\n",
            "answer: 9\n",
            "depends: 2\n",
            "prior: 5\n",
            "continuous: 5\n",
            "fractional: 1\n",
            "height: 6\n",
            "duplicate: 1\n",
            "setup: 1\n",
            "treated: 2\n",
            "identically: 3\n",
            "rest: 1\n",
            "being: 23\n",
            "tested: 12\n",
            "represent: 8\n",
            "what’s: 1\n",
            "unchanged: 1\n",
            "wante: 1\n",
            "adding: 3\n",
            "fertilizer: 4\n",
            "plant’s: 1\n",
            "soil: 1\n",
            "growth: 2\n",
            "plant: 2\n",
            "efforts: 1\n",
            "remove: 1\n",
            "ffect: 1\n",
            "either: 9\n",
            "thus: 4\n",
            "conditions: 13\n",
            "uniform: 2\n",
            "so: 13\n",
            "isolate: 1\n",
            "possible: 8\n",
            "controls: 1\n",
            "often: 17\n",
            "implemented: 1\n",
            "variabl: 1\n",
            "interferes: 3\n",
            "held: 3\n",
            "impact: 8\n",
            "removed: 3\n",
            "better: 8\n",
            "analyze: 9\n",
            "varia: 1\n",
            "ble: 2\n",
            "controlled: 5\n",
            "scientific: 10\n",
            "termed: 2\n",
            "manipulated: 7\n",
            "reveal: 3\n",
            "responding: 1\n",
            "wh: 2\n",
            "ile: 1\n",
            "system: 5\n",
            "fixed: 5\n",
            "convenience: 3\n",
            "easily: 3\n",
            "accidental: 1\n",
            "strategy: 8\n",
            "uses: 10\n",
            "participate: 6\n",
            "purposivepurposeful: 1\n",
            "considered: 6\n",
            "sometimes: 5\n",
            "judgmental: 2\n",
            "generalized: 8\n",
            "convergent: 1\n",
            "ratings: 2\n",
            "theoretically: 2\n",
            "cooperation: 1\n",
            "completed: 4\n",
            "interviews: 10\n",
            "contacted: 2\n",
            "capable: 1\n",
            "interviewed: 2\n",
            "core: 1\n",
            "category: 4\n",
            "integrate: 1\n",
            "correlation: 12\n",
            "abbreviated: 1\n",
            "pa: 1\n",
            "irs: 1\n",
            "interval: 11\n",
            "range: 6\n",
            "100: 4\n",
            "zero: 4\n",
            "associated: 7\n",
            "positively: 3\n",
            "correla: 1\n",
            "tend: 2\n",
            "increase: 4\n",
            "correlated: 4\n",
            "because: 13\n",
            "increases: 3\n",
            "tends: 1\n",
            "negatively: 1\n",
            "decreases: 1\n",
            "correlat: 1\n",
            "ional: 1\n",
            "collecting: 7\n",
            "exists: 5\n",
            "counterbalanced: 1\n",
            "receive: 2\n",
            "treat: 1\n",
            "ments: 2\n",
            "after: 9\n",
            "covariate: 1\n",
            "product: 2\n",
            "times: 8\n",
            "standard: 24\n",
            "deviations: 4\n",
            "m: 3\n",
            "coverage: 1\n",
            "selecting: 7\n",
            "reflect: 4\n",
            "wish: 1\n",
            "credibility: 2\n",
            "ability: 6\n",
            "demonstrate: 4\n",
            "object: 3\n",
            "accurately: 3\n",
            "was: 18\n",
            "conducted: 20\n",
            "been: 13\n",
            "demonstrated: 1\n",
            "instrumental: 2\n",
            "predicted: 6\n",
            "prediction: 12\n",
            "assess: 7\n",
            "referenced: 3\n",
            "specifies: 2\n",
            "icular: 1\n",
            "goal: 6\n",
            "students: 5\n",
            "achieve: 2\n",
            "requires: 2\n",
            "relevant: 5\n",
            "‘gold: 1\n",
            "standard’: 1\n",
            "reliable: 5\n",
            "provide: 3\n",
            "check: 1\n",
            "new: 13\n",
            "ie: 5\n",
            "compare: 4\n",
            "critical: 10\n",
            "raise: 1\n",
            "philosophical: 2\n",
            "ethical: 1\n",
            "sampl: 2\n",
            "enlightening: 1\n",
            "unusual: 1\n",
            "approach: 8\n",
            "social: 12\n",
            "germanys: 1\n",
            "neo: 1\n",
            "marxist: 1\n",
            "“frankfurt: 1\n",
            "school”: 1\n",
            "aims: 3\n",
            "criticize: 1\n",
            "society: 4\n",
            "opposing: 2\n",
            "political: 4\n",
            "orthodoxy: 1\n",
            "modern: 1\n",
            "communism: 1\n",
            "promote: 2\n",
            "emancipatory: 1\n",
            "forces: 1\n",
            "expose: 1\n",
            "impede: 1\n",
            "them: 5\n",
            "explains: 4\n",
            "meanings: 1\n",
            "actions: 3\n",
            "influenced: 3\n",
            "per: 2\n",
            "son’s: 1\n",
            "sectional: 2\n",
            "point: 16\n",
            "contrasted: 1\n",
            "tabulation: 1\n",
            "display: 4\n",
            "across: 4\n",
            "top: 1\n",
            "second: 4\n",
            "side: 2\n",
            "numbers: 8\n",
            "correspond: 2\n",
            "cell: 1\n",
            "indicated: 2\n",
            "cells: 1\n",
            "validation: 2\n",
            "equation: 11\n",
            "least: 11\n",
            "tha: 3\n",
            "crystallization: 1\n",
            "occasions: 1\n",
            "kinds: 2\n",
            "fall: 8\n",
            "place: 5\n",
            "make: 6\n",
            "coherent: 1\n",
            "picture: 1\n",
            "sum: 2\n",
            "curvilinear: 2\n",
            "linear: 8\n",
            "plotted: 4\n",
            "forms: 5\n",
            "scatterplot: 3\n",
            "line: 3\n",
            "fits: 1\n",
            "straight: 2\n",
            "factual: 3\n",
            "measurements: 6\n",
            "reasoning: 10\n",
            "discussion: 4\n",
            "calculation: 2\n",
            "organized: 3\n",
            "understand: 6\n",
            "targ: 1\n",
            "et: 1\n",
            "umbrella: 1\n",
            "cost: 2\n",
            "benefit: 2\n",
            "network: 2\n",
            "path: 4\n",
            "regression: 21\n",
            "simplifying: 1\n",
            "comprehensible: 1\n",
            "book: 2\n",
            "logbook: 1\n",
            "work: 6\n",
            "done: 5\n",
            "includes: 7\n",
            "reactions: 1\n",
            "recording: 1\n",
            "imputation: 6\n",
            "fill: 3\n",
            "missing: 13\n",
            "due: 20\n",
            "nonresponse: 4\n",
            "careful: 3\n",
            "imputations: 1\n",
            "hot: 1\n",
            "deck: 2\n",
            "cold: 1\n",
            "allow: 5\n",
            "were: 8\n",
            "partially: 2\n",
            "mining: 1\n",
            "analyzing: 3\n",
            "perspectives: 2\n",
            "summarizing: 2\n",
            "useful: 4\n",
            "discover: 2\n",
            "colle: 1\n",
            "cted: 3\n",
            "meet: 1\n",
            "standards: 1\n",
            "trustworthy: 1\n",
            "dependable: 1\n",
            "saturation: 3\n",
            "cease: 1\n",
            "closure: 1\n",
            "arrived: 2\n",
            "info: 1\n",
            "rmation: 1\n",
            "shared: 2\n",
            "becomes: 2\n",
            "repetitive: 1\n",
            "contains: 2\n",
            "reasonably: 1\n",
            "confident: 1\n",
            "inclusion: 1\n",
            "additional: 3\n",
            "unlikely: 3\n",
            "generate: 1\n",
            "simply: 3\n",
            "deception: 1\n",
            "intentionally: 1\n",
            "misleading: 1\n",
            "withholding: 1\n",
            "nature: 8\n",
            "deduction: 1\n",
            "deductive: 2\n",
            "begins: 4\n",
            "heory: 2\n",
            "generation: 1\n",
            "ultimately: 3\n",
            "lead: 4\n",
            "confirmation: 1\n",
            "lack: 5\n",
            "thereof: 1\n",
            "logical: 3\n",
            "developing: 3\n",
            "predictions: 2\n",
            "hypothe: 1\n",
            "ses: 1\n",
            "principles: 4\n",
            "moves: 2\n",
            "degrees: 3\n",
            "freedom: 3\n",
            "parameter: 3\n",
            "models: 11\n",
            "generally: 14\n",
            "preferred: 1\n",
            "offer: 1\n",
            "simpler: 1\n",
            "demographics: 1\n",
            "ethnicity: 1\n",
            "demonstration: 3\n",
            "retests: 1\n",
            "already: 1\n",
            "someone: 5\n",
            "show: 1\n",
            "something: 3\n",
            "dependability: 3\n",
            "able: 5\n",
            "changes: 6\n",
            "changing: 2\n",
            "surrounding: 4\n",
            "condi: 1\n",
            "tions: 1\n",
            "varies: 4\n",
            "“depends”: 1\n",
            "obtaine: 1\n",
            "aid: 1\n",
            "interpretation: 6\n",
            "relative: 5\n",
            "descriptive: 9\n",
            "field: 22\n",
            "summarize: 2\n",
            "dispersion: 7\n",
            "deviation: 15\n",
            "ran: 3\n",
            "ge: 1\n",
            "descriptors: 1\n",
            "sources: 5\n",
            "flexibility: 1\n",
            "observational: 6\n",
            "st: 2\n",
            "udy: 2\n",
            "pursue: 1\n",
            "inquiries: 2\n",
            "topics: 7\n",
            "emerge: 2\n",
            "determinism: 1\n",
            "belief: 2\n",
            "everything: 1\n",
            "caused: 5\n",
            "specified: 8\n",
            "antecedent: 1\n",
            "predictable: 1\n",
            "haphazardly: 1\n",
            "ass: 1\n",
            "umption: 1\n",
            "paradigm: 6\n",
            "distance: 6\n",
            "dichotomous: 5\n",
            "male: 1\n",
            "female: 1\n",
            "direct: 10\n",
            "effe: 1\n",
            "ct: 1\n",
            "intervening: 2\n",
            "gathering: 4\n",
            "primarily: 2\n",
            "close: 3\n",
            "visual: 2\n",
            "inspection: 1\n",
            "natural: 6\n",
            "setting: 12\n",
            "involve: 2\n",
            "actively: 2\n",
            "engaging: 2\n",
            "ting: 1\n",
            "conversations: 2\n",
            "observer: 7\n",
            "strives: 1\n",
            "unobtrusive: 2\n",
            "detached: 1\n",
            "directional: 2\n",
            "relational: 1\n",
            "stated: 3\n",
            "manner: 3\n",
            "direction: 4\n",
            "greater: 5\n",
            "less: 4\n",
            "han: 1\n",
            "hypothesized: 1\n",
            "disconfirming: 1\n",
            "open: 8\n",
            "differs: 1\n",
            "consensus: 1\n",
            "discourse: 4\n",
            "c: 7\n",
            "ommunity: 1\n",
            "community: 10\n",
            "scholars: 2\n",
            "communicate: 2\n",
            "published: 3\n",
            "articles: 3\n",
            "communitys: 2\n",
            "journals: 2\n",
            "presentations: 1\n",
            "conventions: 4\n",
            "adhere: 2\n",
            "certai: 1\n",
            "theories: 10\n",
            "present: 4\n",
            "ations: 2\n",
            "solely: 2\n",
            "whole: 4\n",
            "siblings: 1\n",
            "assume: 2\n",
            "indivisible: 1\n",
            "opposite: 3\n",
            "discriminate: 1\n",
            "sh: 1\n",
            "ould: 2\n",
            "discriminates: 1\n",
            "grouping: 3\n",
            "identifies: 2\n",
            "distinguish: 1\n",
            "discriminant: 1\n",
            "seek: 1\n",
            "child: 5\n",
            "subsidies: 1\n",
            "do: 10\n",
            "spread: 3\n",
            "techniques: 9\n",
            "skew: 1\n",
            "occ: 1\n",
            "ur: 1\n",
            "listed: 3\n",
            "bottom: 1\n",
            "occurs: 10\n",
            "drawn: 5\n",
            "corresponds: 2\n",
            "occurred: 4\n",
            "double: 3\n",
            "barreled: 1\n",
            "erroneously: 2\n",
            "blind: 2\n",
            "experimenter: 1\n",
            "unawa: 1\n",
            "dummy: 4\n",
            "turned: 1\n",
            "own: 8\n",
            "coded: 4\n",
            "0: 7\n",
            "1: 5\n",
            "2: 2\n",
            "duratio: 1\n",
            "length: 3\n",
            "dynamic: 1\n",
            "forward: 1\n",
            "right: 3\n",
            "wrong: 2\n",
            "answers: 3\n",
            "res: 3\n",
            "earcher: 1\n",
            "finding: 1\n",
            "ecological: 2\n",
            "fallacy: 1\n",
            "false: 4\n",
            "assuming: 1\n",
            "infer: 1\n",
            "generalizability: 4\n",
            "environments: 2\n",
            "econometrics: 1\n",
            "economics: 1\n",
            "applies: 1\n",
            "tools: 1\n",
            "inference: 1\n",
            "empirical: 4\n",
            "postulated: 1\n",
            "economic: 2\n",
            "resources: 1\n",
            "center: 1\n",
            "eric: 1\n",
            "size: 13\n",
            "es: 1\n",
            "indicate: 8\n",
            "magnitude: 5\n",
            "attributed: 1\n",
            "manipulations: 2\n",
            "strongly: 3\n",
            "divided: 3\n",
            "deviati: 1\n",
            "pooled: 1\n",
            "alone: 1\n",
            "strength: 2\n",
            "effectiveness: 4\n",
            "patientresearch: 1\n",
            "efficacy: 2\n",
            "produce: 4\n",
            "beneficial: 1\n",
            "duration: 1\n",
            "course: 5\n",
            "disease: 1\n",
            "evaluating: 1\n",
            "clin: 1\n",
            "ical: 1\n",
            "clinical: 3\n",
            "electronic: 2\n",
            "text: 7\n",
            "paper: 4\n",
            "essentially: 1\n",
            "copied: 1\n",
            "medium: 1\n",
            "emic: 2\n",
            "insider: 1\n",
            "empathic: 1\n",
            "eutrality: 1\n",
            "strive: 1\n",
            "compiling: 1\n",
            "systematized: 1\n",
            "gained: 1\n",
            "formula: 1\n",
            "insights: 1\n",
            "generalizations: 1\n",
            "researched: 2\n",
            "endogeneity: 2\n",
            "threat: 17\n",
            "exogenous: 1\n",
            "endogenous: 1\n",
            "endpoint: 1\n",
            "protocol: 4\n",
            "designed: 6\n",
            "evaluate: 2\n",
            "endpoints: 1\n",
            "severe: 1\n",
            "toxicity: 1\n",
            "disea: 1\n",
            "se: 1\n",
            "progression: 1\n",
            "death: 1\n",
            "epistemology: 1\n",
            "concerns: 1\n",
            "construction: 2\n",
            "asks: 5\n",
            "constitutes: 1\n",
            "validated: 1\n",
            "equitable: 1\n",
            "fair: 1\n",
            "just: 3\n",
            "burdens: 1\n",
            "esearch: 1\n",
            "fairly: 1\n",
            "distributed: 3\n",
            "equivalency: 1\n",
            "items: 8\n",
            "identical: 3\n",
            "difficulty: 1\n",
            "equivalent: 4\n",
            "unexplained: 1\n",
            "residuals: 1\n",
            "estimated: 5\n",
            "calculated: 5\n",
            "errors: 3\n",
            "inconsistency: 1\n",
            "alternate: 1\n",
            "predicta: 1\n",
            "built: 1\n",
            "accompanies: 1\n",
            "quantity: 2\n",
            "ethics: 1\n",
            "morality: 1\n",
            "ethnogr: 1\n",
            "aphic: 1\n",
            "examining: 5\n",
            "circumstances: 4\n",
            "edm: 1\n",
            "tree: 1\n",
            "flow: 1\n",
            "chart: 2\n",
            "comprises: 1\n",
            "series: 5\n",
            "nested: 7\n",
            "¿if: 1\n",
            "then¿: 1\n",
            "link: 3\n",
            "criteria: 3\n",
            "combinations: 2\n",
            "crite: 1\n",
            "ria: 1\n",
            "ethnographic: 2\n",
            "toface: 3\n",
            "explore: 6\n",
            "great: 3\n",
            "depth: 4\n",
            "customized: 2\n",
            "probed: 2\n",
            "extensively: 3\n",
            "anthropology: 1\n",
            "sociology: 1\n",
            "unde: 1\n",
            "rstand: 1\n",
            "natives’: 1\n",
            "insiders’: 1\n",
            "world: 3\n",
            "ethnographyethnographic: 1\n",
            "extended: 2\n",
            "period: 9\n",
            "inter: 3\n",
            "ethnomethodology: 2\n",
            "activities: 7\n",
            "sense: 1\n",
            "surroundings: 1\n",
            "etic: 3\n",
            "ethnographers: 1\n",
            "refer: 2\n",
            "outsider’s: 1\n",
            "spe: 1\n",
            "cific: 1\n",
            "outsider: 1\n",
            "cultures: 1\n",
            "evaluation: 5\n",
            "monitor: 1\n",
            "implementation: 4\n",
            "rograms: 1\n",
            "effectively: 1\n",
            "practices: 2\n",
            "goals: 3\n",
            "existence: 3\n",
            "must: 8\n",
            "decide: 3\n",
            "heshe: 2\n",
            "going: 3\n",
            "coun: 1\n",
            "once: 4\n",
            "matter: 1\n",
            "count: 1\n",
            "damn: 1\n",
            "counted: 2\n",
            "even: 4\n",
            "though: 2\n",
            "50: 5\n",
            "latter: 1\n",
            "measureme: 1\n",
            "nt: 2\n",
            "interested: 2\n",
            "whereas: 1\n",
            "former: 1\n",
            "looking: 1\n",
            "expectancy: 3\n",
            "unconscious: 1\n",
            "conscious: 2\n",
            "cues: 1\n",
            "convey: 1\n",
            "wants: 1\n",
            "expecting: 1\n",
            "behave: 1\n",
            "minimized: 1\n",
            "interactions: 2\n",
            "automated: 1\n",
            "proto: 1\n",
            "cols: 1\n",
            "sorts: 1\n",
            "things: 7\n",
            "doing: 2\n",
            "experime: 1\n",
            "varied: 1\n",
            "variation: 6\n",
            "hold: 1\n",
            "investigat: 1\n",
            "ion: 2\n",
            "establish: 2\n",
            "andeffect: 1\n",
            "randomization: 2\n",
            "dom: 1\n",
            "allocation: 1\n",
            "introduction: 1\n",
            "der: 1\n",
            "wi: 1\n",
            "thin: 1\n",
            "creates: 1\n",
            "observe: 2\n",
            "interpret: 1\n",
            "element: 1\n",
            "create: 2\n",
            "origin: 1\n",
            "differently: 2\n",
            "ne: 1\n",
            "altered: 3\n",
            "inquiry: 9\n",
            "focuses: 4\n",
            "formulating: 1\n",
            "hypotheses: 6\n",
            "subsequent: 2\n",
            "dat: 1\n",
            "exploratory: 2\n",
            "further: 1\n",
            "clarified: 1\n",
            "expect: 2\n",
            "relations: 2\n",
            "variety: 3\n",
            "review: 9\n",
            "interpretatio: 1\n",
            "criticism: 3\n",
            "genuineness: 1\n",
            "document: 5\n",
            "historical: 3\n",
            "transferable: 2\n",
            "beyond: 5\n",
            "extraneous: 4\n",
            "therefore: 1\n",
            "needs: 1\n",
            "makes: 2\n",
            "uncontrolled: 2\n",
            "extrapola: 1\n",
            "tion: 3\n",
            "predicting: 2\n",
            "projecting: 1\n",
            "explores: 3\n",
            "factorial: 2\n",
            "falsification: 1\n",
            "mislead: 1\n",
            "ask: 3\n",
            "feel: 1\n",
            "diary: 1\n",
            "events: 16\n",
            "she: 5\n",
            "comes: 1\n",
            "cont: 2\n",
            "jottings: 1\n",
            "quick: 2\n",
            "ethnographer: 2\n",
            "log: 1\n",
            "running: 1\n",
            "plans: 2\n",
            "spend: 1\n",
            "his: 6\n",
            "detail: 2\n",
            "conv: 1\n",
            "ersations: 1\n",
            "recorded: 8\n",
            "principle: 2\n",
            "live: 3\n",
            "take: 5\n",
            "academic: 2\n",
            "investigative: 1\n",
            "undertaken: 2\n",
            "laboratories: 1\n",
            "classrooms: 3\n",
            "structured: 4\n",
            "observing: 2\n",
            "communities: 1\n",
            "long: 1\n",
            "relocation: 1\n",
            "takes: 4\n",
            "fivenumber: 1\n",
            "lowest: 3\n",
            "quartile: 2\n",
            "third: 2\n",
            "overview: 1\n",
            "variability: 3\n",
            "shape: 1\n",
            "five: 2\n",
            "eliminate: 1\n",
            "omission: 1\n",
            "unmeasured: 1\n",
            "eliminated: 1\n",
            "intercep: 1\n",
            "floor: 2\n",
            "below: 4\n",
            "lower: 6\n",
            "flowchart: 1\n",
            "tally: 2\n",
            "sheets: 1\n",
            "remarks: 2\n",
            "focus: 9\n",
            "particula: 2\n",
            "uncover: 1\n",
            "exchange: 1\n",
            "interv: 1\n",
            "iew: 1\n",
            "hear: 1\n",
            "roundtable: 1\n",
            "charged: 1\n",
            "problems: 5\n",
            "options: 2\n",
            "solutions: 1\n",
            "consist: 2\n",
            "4: 2\n",
            "12: 1\n",
            "guided: 1\n",
            "moderators: 1\n",
            "keep: 2\n",
            "flowing: 1\n",
            "collect: 3\n",
            "forecasting: 1\n",
            "unemployment: 1\n",
            "next: 1\n",
            "foreshadowed: 1\n",
            "serves: 2\n",
            "framework: 4\n",
            "la: 1\n",
            "unching: 1\n",
            "guidelines: 2\n",
            "investigating: 2\n",
            "tabular: 1\n",
            "showing: 1\n",
            "gain: 3\n",
            "pretest: 8\n",
            "posttest: 8\n",
            "references: 4\n",
            "indexes: 1\n",
            "abstracts: 1\n",
            "situations: 1\n",
            "applied: 4\n",
            "gini: 1\n",
            "inequality: 2\n",
            "racial: 1\n",
            "gis: 1\n",
            "geographical: 1\n",
            "enables: 2\n",
            "assemble: 1\n",
            "store: 1\n",
            "manipulate: 1\n",
            "grade: 3\n",
            "diagram: 1\n",
            "illustrates: 1\n",
            "along: 3\n",
            "axes: 3\n",
            "positioned: 1\n",
            "angles: 1\n",
            "practice: 3\n",
            "add: 1\n",
            "insight: 1\n",
            "why: 3\n",
            "exist: 4\n",
            "inductive: 6\n",
            "generated: 1\n",
            "ind: 1\n",
            "ividual: 1\n",
            "membership: 1\n",
            "hawthorne: 2\n",
            "positive: 5\n",
            "interpretations: 2\n",
            "resu: 1\n",
            "lting: 1\n",
            "feeling: 2\n",
            "receiving: 1\n",
            "special: 6\n",
            "attention: 2\n",
            "heterogeneity: 1\n",
            "dissimilarity: 1\n",
            "respect: 5\n",
            "hierarchica: 1\n",
            "layers: 1\n",
            "sports: 2\n",
            "team: 2\n",
            "player: 1\n",
            "hierarchical: 2\n",
            "modeling: 5\n",
            "hlm: 2\n",
            "multi: 2\n",
            "estimating: 5\n",
            "hildren: 1\n",
            "schools: 2\n",
            "formulate: 2\n",
            "partition: 1\n",
            "covariance: 1\n",
            "components: 2\n",
            "histogram: 1\n",
            "consisting: 2\n",
            "rectangles: 1\n",
            "rectangle: 1\n",
            "relate: 1\n",
            "occurrences: 1\n",
            "trends: 3\n",
            "anticipate: 1\n",
            "history: 3\n",
            "possibility: 10\n",
            "whic: 1\n",
            "affect: 3\n",
            "thereby: 8\n",
            "affecting: 4\n",
            "holistic: 1\n",
            "almost: 1\n",
            "communication: 3\n",
            "homogeneous: 1\n",
            "samp: 1\n",
            "hypertext: 3\n",
            "sequential: 1\n",
            "composed: 1\n",
            "links: 2\n",
            "nodes: 1\n",
            "tentative: 4\n",
            "caus: 1\n",
            "proposed: 2\n",
            "hasn’t: 1\n",
            "yet: 2\n",
            "rigorously: 1\n",
            "ac: 1\n",
            "cepted: 1\n",
            "testable: 1\n",
            "regarding: 3\n",
            "occu: 1\n",
            "rrence: 1\n",
            "rejected: 3\n",
            "hypoth: 1\n",
            "esis: 1\n",
            "sets: 3\n",
            "prove: 1\n",
            "disprove: 1\n",
            "attend: 2\n",
            "coping: 2\n",
            "skills: 2\n",
            "statem: 2\n",
            "negative: 4\n",
            "stud: 3\n",
            "imputed: 1\n",
            "filled: 1\n",
            "analyst: 1\n",
            "incidence: 1\n",
            "disorder: 5\n",
            "investigated: 3\n",
            "risk: 1\n",
            "independence: 1\n",
            "annual: 2\n",
            "snow: 2\n",
            "yankees: 1\n",
            "season: 1\n",
            "coat: 1\n",
            "sales: 1\n",
            "iid: 1\n",
            "x1: 1\n",
            "x2: 1\n",
            "interpreted: 2\n",
            "one’s: 1\n",
            "vari: 2\n",
            "impacted: 1\n",
            "impacts: 1\n",
            "earlier: 1\n",
            "indepth: 1\n",
            "composite: 3\n",
            "summarizes: 1\n",
            "several: 10\n",
            "dimension: 1\n",
            "summed: 2\n",
            "var: 2\n",
            "iables: 2\n",
            "assumed: 3\n",
            "underlying: 3\n",
            "indicator: 2\n",
            "properties: 3\n",
            "indicators: 2\n",
            "progress: 2\n",
            "toward: 2\n",
            "intended: 6\n",
            "outputs: 1\n",
            "outco: 1\n",
            "mes: 1\n",
            "objectives: 1\n",
            "indirect: 2\n",
            "indirectly: 1\n",
            "income: 5\n",
            "wage: 1\n",
            "rates: 2\n",
            "individualism: 1\n",
            "policy: 4\n",
            "primary: 5\n",
            "regard: 2\n",
            "liberty: 1\n",
            "rights: 1\n",
            "conclusion: 3\n",
            "formulated: 1\n",
            "instances: 1\n",
            "starts: 1\n",
            "formulates: 1\n",
            "throughout: 1\n",
            "induc: 1\n",
            "tive: 1\n",
            "regularities: 1\n",
            "detected: 1\n",
            "formulation: 1\n",
            "develop: 5\n",
            "rules: 4\n",
            "inferential: 5\n",
            "det: 1\n",
            "ermining: 1\n",
            "likely: 3\n",
            "entire: 7\n",
            "informal: 2\n",
            "lessstructured: 1\n",
            "sequence: 3\n",
            "questioning: 2\n",
            "resemble: 1\n",
            "casual: 1\n",
            "conversation: 1\n",
            "informed: 3\n",
            "parties: 1\n",
            "obtaining: 2\n",
            "voluntary: 2\n",
            "participation: 2\n",
            "risks: 1\n",
            "agre: 1\n",
            "ement: 1\n",
            "impacteffects: 1\n",
            "insiderness: 1\n",
            "access: 1\n",
            "persons: 3\n",
            "places: 1\n",
            "que: 1\n",
            "stionnaire: 1\n",
            "schedule: 2\n",
            "decay: 1\n",
            "instrumentation: 3\n",
            "unclear: 1\n",
            "wording: 1\n",
            "asking: 5\n",
            "unable: 1\n",
            "supply: 3\n",
            "changed: 2\n",
            "little: 2\n",
            "ef: 1\n",
            "fort: 1\n",
            "instruments: 3\n",
            "interaction: 5\n",
            "unique: 2\n",
            "depending: 2\n",
            "v: 3\n",
            "alue: 2\n",
            "interactive: 2\n",
            "maximum: 2\n",
            "conceptualization: 1\n",
            "intercept: 1\n",
            "interjudge: 1\n",
            "scorers: 1\n",
            "raters: 1\n",
            "observers: 4\n",
            "ency: 1\n",
            "determining: 3\n",
            "ontents: 1\n",
            "rigor: 2\n",
            "studys: 2\n",
            "conduct: 4\n",
            "decisions: 2\n",
            "concerning: 3\n",
            "designers: 1\n",
            "explanations: 1\n",
            "assessing: 3\n",
            "vali: 1\n",
            "dity: 1\n",
            "administration: 2\n",
            "rater: 1\n",
            "ated: 1\n",
            "expressed: 2\n",
            "percentage: 7\n",
            "ratersobservers: 1\n",
            "interrater: 1\n",
            "agree: 3\n",
            "addresses: 1\n",
            "rating: 6\n",
            "adjacent: 3\n",
            "arbitrary: 2\n",
            "subtra: 1\n",
            "meaningfully: 1\n",
            "determined: 8\n",
            "distances: 1\n",
            "wherein: 1\n",
            "ance: 2\n",
            "studyresearch: 2\n",
            "introduced: 1\n",
            "subject¿s: 1\n",
            "performed: 3\n",
            "questioned: 1\n",
            "orally: 1\n",
            "involving: 9\n",
            "telephone: 1\n",
            "mistakes: 1\n",
            "influenci: 1\n",
            "ng: 2\n",
            "slightly: 1\n",
            "phrasing: 1\n",
            "tone: 2\n",
            "voice: 1\n",
            "interviewers: 3\n",
            "intentional: 1\n",
            "cheating: 1\n",
            "fraudulent: 1\n",
            "entry: 1\n",
            "intrinsic: 1\n",
            "attempts: 3\n",
            "investigator: 4\n",
            "investigators: 6\n",
            "principal: 3\n",
            "coprincipal: 1\n",
            "always: 1\n",
            "co: 1\n",
            "irrelevant: 1\n",
            "ones: 5\n",
            "deleting: 1\n",
            "skipping: 1\n",
            "unwanted: 1\n",
            "viewing: 1\n",
            "reexamine: 1\n",
            "reassess: 1\n",
            "perhaps: 2\n",
            "eve: 1\n",
            "alter: 2\n",
            "scheme: 2\n",
            "jackknife: 2\n",
            "technique: 10\n",
            "parameters: 3\n",
            "gauge: 1\n",
            "uncertainty: 1\n",
            "estimates: 4\n",
            "name: 3\n",
            "cut: 1\n",
            "knife: 1\n",
            "get: 2\n",
            "rationale: 3\n",
            "implications: 2\n",
            "informants: 1\n",
            "expert: 2\n",
            "kinesics: 2\n",
            "examines: 1\n",
            "communicated: 1\n",
            "body: 1\n",
            "movement: 1\n",
            "possesses: 1\n",
            "kurtosis: 3\n",
            "me: 3\n",
            "asures: 1\n",
            "peaked: 2\n",
            "flatter: 1\n",
            "leading: 1\n",
            "quest: 1\n",
            "phrased: 1\n",
            "leads: 2\n",
            "interviewee: 1\n",
            "squares: 2\n",
            "commonly: 7\n",
            "calculating: 4\n",
            "minimizes: 2\n",
            "tween: 1\n",
            "chosen: 3\n",
            "word: 3\n",
            "phrases: 2\n",
            "carley: 1\n",
            "500: 1\n",
            "su: 1\n",
            "fficient: 1\n",
            "confidence: 6\n",
            "contain: 2\n",
            "corresponding: 5\n",
            "com: 1\n",
            "monly: 1\n",
            "95: 1\n",
            "99: 1\n",
            "percent: 4\n",
            "generalization: 3\n",
            "coll: 1\n",
            "apsed: 1\n",
            "implication: 1\n",
            "code: 2\n",
            "explicit: 2\n",
            "appearances: 1\n",
            "implied: 2\n",
            "eventevents: 1\n",
            "told: 1\n",
            "increasingly: 1\n",
            "aud: 1\n",
            "io: 1\n",
            "video: 1\n",
            "hisher: 3\n",
            "story: 1\n",
            "covers: 1\n",
            "longer: 1\n",
            "span: 1\n",
            "likert: 1\n",
            "attitudes: 5\n",
            "indicating: 6\n",
            "disagreement: 1\n",
            "limitation: 1\n",
            "aspect: 1\n",
            "researche: 1\n",
            "decrease: 2\n",
            "pointer: 1\n",
            "node: 2\n",
            "comprehensive: 2\n",
            "beginning: 1\n",
            "identification: 1\n",
            "location: 3\n",
            "containing: 1\n",
            "producing: 4\n",
            "logic: 1\n",
            "logistic: 1\n",
            "malefemale: 1\n",
            "logit: 3\n",
            "longitud: 1\n",
            "inal: 1\n",
            "considerable: 1\n",
            "months: 1\n",
            "years: 3\n",
            "obvious: 2\n",
            "nonparametric: 3\n",
            "uncorrelated: 1\n",
            "manova: 1\n",
            "multivariate: 2\n",
            "varying: 1\n",
            "margin: 1\n",
            "permittable: 1\n",
            "acceptable: 2\n",
            "allowance: 1\n",
            "slight: 1\n",
            "miscalculation: 1\n",
            "changin: 1\n",
            "g: 2\n",
            "matched: 4\n",
            "paired: 3\n",
            "explicitly: 2\n",
            "iq: 1\n",
            "attribute: 2\n",
            "twice: 3\n",
            "repeated: 3\n",
            "pairs: 1\n",
            "icipants: 1\n",
            "sort: 2\n",
            "write: 1\n",
            "essays: 1\n",
            "writing: 2\n",
            "class: 4\n",
            "matching: 5\n",
            "columns: 1\n",
            "required: 2\n",
            "match: 2\n",
            "left: 2\n",
            "column: 2\n",
            "closely: 3\n",
            "equating: 2\n",
            "counterpart: 1\n",
            "equally: 2\n",
            "feature: 2\n",
            "maturation: 1\n",
            "passage: 1\n",
            "maxima: 2\n",
            "function: 2\n",
            "maximal: 1\n",
            "ample: 2\n",
            "diversity: 1\n",
            "ave: 1\n",
            "raging: 1\n",
            "absolute: 1\n",
            "meanarithmetic: 1\n",
            "tenden: 1\n",
            "cy: 1\n",
            "possess: 2\n",
            "‘true’: 1\n",
            "indices: 3\n",
            "attained: 2\n",
            "researc: 1\n",
            "mechanical: 1\n",
            "pairing: 1\n",
            "checking: 1\n",
            "restating: 1\n",
            "paraphrasing: 1\n",
            "received: 1\n",
            "ensure: 5\n",
            "correct: 1\n",
            "mod: 1\n",
            "els: 1\n",
            "interrelated: 1\n",
            "subconscious: 1\n",
            "perceptions: 2\n",
            "networks: 2\n",
            "draw: 2\n",
            "inferences: 3\n",
            "gather: 4\n",
            "mentor: 1\n",
            "experi: 1\n",
            "enced: 1\n",
            "trusted: 1\n",
            "adviser: 1\n",
            "advice: 1\n",
            "counseling: 1\n",
            "meta: 1\n",
            "combines: 1\n",
            "analyzes: 1\n",
            "slurring: 1\n",
            "comb: 1\n",
            "ine: 1\n",
            "approaches: 4\n",
            "adequately: 3\n",
            "epistemological: 2\n",
            "origins: 1\n",
            "underpin: 1\n",
            "methodologies: 2\n",
            "proceed: 1\n",
            "steps: 1\n",
            "application: 5\n",
            "modes: 1\n",
            "employed: 2\n",
            "discipline: 2\n",
            "metropolitan: 2\n",
            "msa: 1\n",
            "us: 2\n",
            "bureau: 1\n",
            "designate: 1\n",
            "counties: 1\n",
            "eng: 1\n",
            "land: 2\n",
            "defined: 2\n",
            "msas: 1\n",
            "labor: 1\n",
            "markets: 1\n",
            "look: 1\n",
            "city: 1\n",
            "minima: 2\n",
            "ar: 1\n",
            "completely: 2\n",
            "mcar: 2\n",
            "implies: 1\n",
            "likelyunlikely: 1\n",
            "approximately: 1\n",
            "unbiased: 3\n",
            "ignore: 1\n",
            "restrict: 1\n",
            "implicitly: 1\n",
            "invokes: 1\n",
            "strong: 1\n",
            "refusal: 2\n",
            "incorrectly: 1\n",
            "apply: 1\n",
            "likelihood: 2\n",
            "strategies: 2\n",
            "imp: 1\n",
            "utation: 1\n",
            "misspecification: 4\n",
            "incorrect: 3\n",
            "combining: 1\n",
            "blended: 1\n",
            "combined: 1\n",
            "methodological: 1\n",
            "triangulation: 3\n",
            "hat: 2\n",
            "ages: 2\n",
            "21: 1\n",
            "33: 3\n",
            "45: 1\n",
            "76: 1\n",
            "modal: 2\n",
            "physical: 1\n",
            "analogy: 1\n",
            "helps: 3\n",
            "successful: 2\n",
            "unexpected: 1\n",
            "experimentally: 1\n",
            "representations: 1\n",
            "imitation: 1\n",
            "emulation: 1\n",
            "moderator: 1\n",
            "mortality: 1\n",
            "whatever: 2\n",
            "reason: 3\n",
            "lost: 1\n",
            "remain: 1\n",
            "absence: 2\n",
            "moving: 1\n",
            "¿smoothed¿: 1\n",
            "seasonal: 1\n",
            "cyclical: 1\n",
            "multilevel: 3\n",
            "hierarchy: 2\n",
            "classes: 2\n",
            "teacher: 1\n",
            "solve: 2\n",
            "dealing: 1\n",
            "hierarchically: 1\n",
            "employs: 1\n",
            "multinomial: 5\n",
            "arises: 2\n",
            "binomial: 1\n",
            "ordered: 4\n",
            "ordinal: 6\n",
            "multinomia: 1\n",
            "never: 1\n",
            "married: 2\n",
            "divorced: 1\n",
            "combination: 2\n",
            "recognition: 1\n",
            "acceptance: 1\n",
            "afte: 1\n",
            "sequentially: 1\n",
            "until: 1\n",
            "settings: 4\n",
            "examine: 2\n",
            "simultaneously: 2\n",
            "controlling: 2\n",
            "mutually: 5\n",
            "exclusive: 5\n",
            "overlapping: 1\n",
            "we: 2\n",
            "say: 1\n",
            "however: 2\n",
            "doesn¿t: 1\n",
            "narrative: 7\n",
            "confused: 2\n",
            "examined: 3\n",
            "ev: 1\n",
            "interference: 1\n",
            "natu: 1\n",
            "ralistic: 1\n",
            "assumes: 1\n",
            "ly: 2\n",
            "skewed: 3\n",
            "extreme: 3\n",
            "higher: 7\n",
            "connected: 1\n",
            "classifies: 1\n",
            "elements: 2\n",
            "color: 1\n",
            "nondirectional: 1\n",
            "specifying: 1\n",
            "exact: 1\n",
            "nonequivalent: 2\n",
            "pretested: 6\n",
            "roups: 1\n",
            "posttested: 3\n",
            "canno: 1\n",
            "met: 2\n",
            "nonparticipant: 1\n",
            "nonrandom: 1\n",
            "samplesampling: 1\n",
            "chance: 14\n",
            "portion: 1\n",
            "nonsampling: 2\n",
            "phase: 1\n",
            "mis: 1\n",
            "nonsignificant: 1\n",
            "sufficient: 3\n",
            "conclude: 1\n",
            "had: 1\n",
            "norm: 5\n",
            "usual: 1\n",
            "high: 1\n",
            "graduation: 1\n",
            "18: 2\n",
            "old: 2\n",
            "graduate: 2\n",
            "younger: 1\n",
            "older: 1\n",
            "ed: 2\n",
            "norms: 2\n",
            "formed: 3\n",
            "selec: 1\n",
            "represented: 4\n",
            "wide: 1\n",
            "portray: 1\n",
            "permits: 1\n",
            "statist: 1\n",
            "ics: 1\n",
            "permit: 1\n",
            "meaningful: 2\n",
            "proposition: 1\n",
            "statistically: 5\n",
            "aning: 1\n",
            "hope: 1\n",
            "allowing: 1\n",
            "reject: 4\n",
            "hy: 1\n",
            "pothesis: 1\n",
            "hypo: 1\n",
            "thesis: 1\n",
            "prejudice: 1\n",
            "obse: 1\n",
            "rved: 1\n",
            "objectively: 1\n",
            "invalid: 1\n",
            "presence: 1\n",
            "expectations: 2\n",
            "odds: 6\n",
            "express: 2\n",
            "od: 1\n",
            "ds: 2\n",
            "ratioor: 1\n",
            "favor: 1\n",
            "reviews: 1\n",
            "incurring: 1\n",
            "non­‐events: 1\n",
            "omitted: 2\n",
            "planations: 1\n",
            "addressed: 1\n",
            "onegroup: 1\n",
            "weak: 3\n",
            "exposed: 3\n",
            "oneshot: 1\n",
            "onetailed: 1\n",
            "tail: 1\n",
            "–: 2\n",
            "oneway: 1\n",
            "anova: 3\n",
            "france: 1\n",
            "england: 1\n",
            "sweden: 1\n",
            "ontology: 1\n",
            "philosophy: 4\n",
            "structures: 2\n",
            "choic: 1\n",
            "truefalse: 1\n",
            "operational: 1\n",
            "defining: 1\n",
            "opinion: 2\n",
            "opportunistic: 1\n",
            "advantage: 1\n",
            "arise: 1\n",
            "oral: 2\n",
            "expression: 1\n",
            "exampl: 1\n",
            "disagree: 2\n",
            "lassification: 1\n",
            "labeling: 1\n",
            "features: 1\n",
            "although: 1\n",
            "letter: 1\n",
            "grades: 1\n",
            "ordinary: 1\n",
            "outlier: 2\n",
            "othe: 1\n",
            "unusually: 2\n",
            "deviate: 3\n",
            "considerably: 1\n",
            "distr: 1\n",
            "ibution: 1\n",
            "pattern: 2\n",
            "oversampling: 2\n",
            "sampled: 3\n",
            "enough: 4\n",
            "yi: 1\n",
            "eld: 1\n",
            "good: 1\n",
            "brought: 1\n",
            "math: 3\n",
            "mat: 1\n",
            "teaching: 2\n",
            "panel: 2\n",
            "intervals: 2\n",
            "kuhn: 1\n",
            "defines: 1\n",
            "constellatio: 1\n",
            "secondly: 1\n",
            "partial: 1\n",
            "correlational: 1\n",
            "ally: 2\n",
            "holding: 1\n",
            "physiological: 1\n",
            "behavioral: 1\n",
            "er: 1\n",
            "develops: 1\n",
            "composition: 2\n",
            "everyday: 1\n",
            "routines: 1\n",
            "rituals: 1\n",
            "alongside: 1\n",
            "member¿s: 1\n",
            "pers: 1\n",
            "pective: 1\n",
            "accomplished: 1\n",
            "asobserver: 1\n",
            "activity: 2\n",
            "revea: 1\n",
            "ls: 1\n",
            "participatory: 1\n",
            "empower: 1\n",
            "bring: 1\n",
            "mul: 1\n",
            "tiple: 1\n",
            "parcel: 1\n",
            "sophisticated: 1\n",
            "connections: 1\n",
            "peer: 2\n",
            "examination: 3\n",
            "replication: 2\n",
            "incorporate: 1\n",
            "peerreview: 1\n",
            "article: 1\n",
            "submits: 1\n",
            "publishing: 1\n",
            "scholarly: 1\n",
            "percentile: 2\n",
            "rank: 3\n",
            "position: 3\n",
            "checklist: 2\n",
            "track: 1\n",
            "task: 2\n",
            "phenomenology: 2\n",
            "roots: 1\n",
            "lived: 1\n",
            "phenomenologyphenomenological: 1\n",
            "commonalities: 1\n",
            "fundamental: 1\n",
            "phonology: 1\n",
            "speech: 1\n",
            "sounds: 1\n",
            "language: 5\n",
            "pie: 2\n",
            "displaying: 1\n",
            "breakdown: 1\n",
            "pile: 1\n",
            "sorting: 1\n",
            "elicit: 1\n",
            "judg: 1\n",
            "domain: 2\n",
            "cards: 1\n",
            "short: 4\n",
            "piles: 1\n",
            "pilot: 1\n",
            "defects: 1\n",
            "poisson: 1\n",
            "spatial: 1\n",
            "arrangements: 1\n",
            "governing: 2\n",
            "serve: 2\n",
            "lternative: 1\n",
            "policies: 1\n",
            "disciplines: 1\n",
            "consideration: 1\n",
            "cle: 1\n",
            "arly: 1\n",
            "fr: 1\n",
            "om: 1\n",
            "portraiture: 1\n",
            "portrayed: 1\n",
            "define: 2\n",
            "papers: 1\n",
            "official: 1\n",
            "organizational: 1\n",
            "viewpoints: 1\n",
            "recommending: 1\n",
            "positivism: 1\n",
            "philosophic: 1\n",
            "viewpoint: 1\n",
            "em: 1\n",
            "phasizing: 1\n",
            "universal: 1\n",
            "bot: 1\n",
            "avoid: 1\n",
            "ii: 3\n",
            "w: 6\n",
            "ill: 1\n",
            "detect: 2\n",
            "powerful: 2\n",
            "populatio: 1\n",
            "practical: 4\n",
            "rast: 1\n",
            "precision: 1\n",
            "tightness: 1\n",
            "limits: 1\n",
            "predefined: 2\n",
            "choice: 1\n",
            "predicts: 1\n",
            "erson: 1\n",
            "att: 1\n",
            "empt: 1\n",
            "inventories: 1\n",
            "deg: 1\n",
            "ree: 1\n",
            "assesses: 2\n",
            "criteri: 2\n",
            "projections: 2\n",
            "predicto: 1\n",
            "modeled: 1\n",
            "pretesting: 1\n",
            "outset: 3\n",
            "effective: 1\n",
            "gr: 2\n",
            "oups: 1\n",
            "react: 1\n",
            "creating: 3\n",
            "ces: 1\n",
            "divides: 1\n",
            "pieces: 1\n",
            "psus: 1\n",
            "firsthand: 1\n",
            "testimony: 1\n",
            "eyewitness: 1\n",
            "reli: 1\n",
            "scientist: 2\n",
            "scholar: 1\n",
            "responsibility: 1\n",
            "ra: 1\n",
            "ndomly: 1\n",
            "ensures: 2\n",
            "prob: 1\n",
            "conventionally: 1\n",
            "rare: 1\n",
            "carrying: 1\n",
            "program: 3\n",
            "projection: 1\n",
            "happen: 2\n",
            "trial: 3\n",
            "protocols: 1\n",
            "developed: 2\n",
            "know: 1\n",
            "eligibility: 1\n",
            "medications: 1\n",
            "dosages: 1\n",
            "proxy: 2\n",
            "¿stand: 1\n",
            "in¿: 1\n",
            "too: 2\n",
            "difficult: 1\n",
            "intends: 3\n",
            "accomplish: 1\n",
            "purposive: 1\n",
            "since: 2\n",
            "pvalue: 2\n",
            "05: 1\n",
            "01: 1\n",
            "10: 2\n",
            "concluding: 1\n",
            "regulations: 1\n",
            "govern: 1\n",
            "nonnumeric: 1\n",
            "transcript: 1\n",
            "unstructured: 2\n",
            "generates: 1\n",
            "largely: 1\n",
            "earch: 1\n",
            "principally: 1\n",
            "conc: 1\n",
            "eptualized: 1\n",
            "distinct: 2\n",
            "continuum: 2\n",
            "quartiles: 1\n",
            "three: 1\n",
            "divide: 1\n",
            "four: 1\n",
            "quasi: 3\n",
            "nonrandomized: 1\n",
            "incorporates: 1\n",
            "transferability: 2\n",
            "compensate: 1\n",
            "designs: 3\n",
            "xperimental: 1\n",
            "environmental: 1\n",
            "influences: 2\n",
            "belongs: 1\n",
            "quixotic: 1\n",
            "consistently: 2\n",
            "yiel: 1\n",
            "quota: 3\n",
            "strata: 2\n",
            "proportionately: 1\n",
            "representativenes: 1\n",
            "recruit: 1\n",
            "polling: 1\n",
            "market: 1\n",
            "assigning: 2\n",
            "domly: 1\n",
            "sign: 2\n",
            "seeing: 1\n",
            "posting: 1\n",
            "20: 1\n",
            "24: 1\n",
            "race: 1\n",
            "ch: 1\n",
            "choose: 1\n",
            "eligible: 3\n",
            "determinate: 1\n",
            "numerically: 1\n",
            "heights: 1\n",
            "taller: 1\n",
            "subtracting: 2\n",
            "hich: 3\n",
            "judgments: 1\n",
            "reco: 1\n",
            "rd: 1\n",
            "quotient: 1\n",
            "converted: 1\n",
            "percentil: 1\n",
            "ranking: 1\n",
            "reflective: 1\n",
            "reflections: 2\n",
            "decline: 1\n",
            "cooperate: 1\n",
            "regressed: 1\n",
            "rovides: 1\n",
            "stable: 2\n",
            "equ: 1\n",
            "ation: 2\n",
            "knowing: 2\n",
            "valu: 1\n",
            "fit: 1\n",
            "coordinate: 2\n",
            "sco: 1\n",
            "regress: 1\n",
            "regardless: 1\n",
            "connection: 1\n",
            "qualities: 1\n",
            "motivation: 1\n",
            "investigati: 1\n",
            "consistent: 3\n",
            "yields: 1\n",
            "trials: 1\n",
            "relics: 1\n",
            "again: 1\n",
            "repetition: 2\n",
            "tudy: 1\n",
            "aspects: 1\n",
            "segments: 1\n",
            "sam: 1\n",
            "ple: 1\n",
            "representativeness: 2\n",
            "identica: 1\n",
            "hyp: 1\n",
            "othesis: 1\n",
            "investigate: 1\n",
            "issue: 3\n",
            "wishes: 2\n",
            "sue: 1\n",
            "lends: 1\n",
            "formal: 2\n",
            "scholarship: 1\n",
            "disciplined: 1\n",
            "hopes: 1\n",
            "contribute: 1\n",
            "nal: 1\n",
            "pertain: 1\n",
            "questionnaires: 1\n",
            "returned: 1\n",
            "interprets: 1\n",
            "produced: 5\n",
            "tables: 2\n",
            "graphs: 2\n",
            "retrospective: 1\n",
            "tries: 1\n",
            "rhetorical: 1\n",
            "entails…1: 1\n",
            "identi: 1\n",
            "fying: 1\n",
            "motivational: 1\n",
            "concern: 1\n",
            "posing: 1\n",
            "heuristic: 1\n",
            "probing: 1\n",
            "fields: 1\n",
            "5: 2\n",
            "justifying: 1\n",
            "lauer: 1\n",
            "asher: 1\n",
            "1988: 1\n",
            "scrupulously: 1\n",
            "meticulously: 1\n",
            "recognize: 1\n",
            "robustness: 1\n",
            "state: 1\n",
            "violated: 1\n",
            "rsquared: 1\n",
            "denotes: 1\n",
            "explained: 1\n",
            "whom: 1\n",
            "transferred: 1\n",
            "designe: 1\n",
            "preferable: 1\n",
            "sm: 1\n",
            "aller: 1\n",
            "successive: 1\n",
            "health: 1\n",
            "authorityhealth: 1\n",
            "board: 1\n",
            "senior: 1\n",
            "managers: 1\n",
            "distortions: 1\n",
            "excluded: 1\n",
            "phone: 3\n",
            "telephones: 1\n",
            "cou: 1\n",
            "ld: 1\n",
            "characterized: 2\n",
            "val: 1\n",
            "ues: 1\n",
            "reduction: 3\n",
            "respon: 1\n",
            "dent: 1\n",
            "alreck: 1\n",
            "454: 1\n",
            "cho: 1\n",
            "sen: 1\n",
            "redundancy: 1\n",
            "confirm: 2\n",
            "expand: 1\n",
            "role: 2\n",
            "scaled: 1\n",
            "transformation: 2\n",
            "scatter: 2\n",
            "plot: 3\n",
            "crosstabulation: 1\n",
            "illustrate: 1\n",
            "public: 1\n",
            "raised: 1\n",
            "supported: 1\n",
            "phenomeno: 1\n",
            "secondary: 1\n",
            "secondhand: 1\n",
            "selective: 2\n",
            "attending: 1\n",
            "reduced: 1\n",
            "indicative: 1\n",
            "vels: 1\n",
            "selfchecklist: 1\n",
            "reads: 1\n",
            "checks: 1\n",
            "engaged: 1\n",
            "semantic: 1\n",
            "differential: 1\n",
            "adjectives: 1\n",
            "separated: 1\n",
            "unlabelled: 1\n",
            "semantics: 1\n",
            "symbols: 1\n",
            "linguistic: 1\n",
            "cuing: 1\n",
            "conn: 1\n",
            "ects: 1\n",
            "stored: 1\n",
            "readers: 2\n",
            "semi: 1\n",
            "stray: 1\n",
            "says: 1\n",
            "assist: 2\n",
            "diagno: 1\n",
            "stic: 2\n",
            "testsignsymptom: 2\n",
            "sensory: 1\n",
            "experienced: 1\n",
            "senses: 1\n",
            "serial: 2\n",
            "establishing: 1\n",
            "accrue: 1\n",
            "biasing: 1\n",
            "quired: 1\n",
            "phrase: 1\n",
            "symbol: 1\n",
            "necessary: 2\n",
            "techniqu: 1\n",
            "populati: 1\n",
            "entirely: 1\n",
            "elementunit: 1\n",
            "sele: 1\n",
            "simulation: 1\n",
            "digits: 1\n",
            "exhibits: 1\n",
            "x: 1\n",
            "axis: 1\n",
            "skewness: 1\n",
            "depart: 1\n",
            "symmetry: 1\n",
            "slope: 1\n",
            "snowball: 1\n",
            "desirability: 1\n",
            "soci: 1\n",
            "desirable: 1\n",
            "functioning: 1\n",
            "societies: 1\n",
            "sociogram: 1\n",
            "enable: 1\n",
            "otherwise: 1\n",
            "complex: 1\n",
            "conceptualize: 1\n",
            "sociolinguistics: 1\n",
            "specifically: 1\n",
            "varieties: 1\n",
            "functions: 1\n",
            "speakers: 1\n",
            "specificity: 1\n",
            "propor: 1\n",
            "diagnostic: 1\n",
            "split: 1\n",
            "half: 2\n",
            "giving: 1\n",
            "scoring: 1\n",
            "spss: 1\n",
            "commercially: 1\n",
            "packages: 1\n",
            "spurious: 1\n",
            "associatio: 1\n",
            "start: 2\n",
            "leaves: 2\n",
            "begin: 1\n",
            "trees: 3\n",
            "leave: 1\n",
            "falling: 2\n",
            "vice: 1\n",
            "versa: 1\n",
            "starting: 1\n",
            "autumn: 1\n",
            "stability: 2\n",
            "root: 1\n",
            "ranges: 1\n",
            "conforms: 1\n",
            "fluctu: 1\n",
            "ates: 1\n",
            "sd: 1\n",
            "calc: 1\n",
            "ulating: 1\n",
            "semeas: 1\n",
            "sed: 1\n",
            "scor: 1\n",
            "expresses: 2\n",
            "far: 1\n",
            "standardization: 2\n",
            "manipulating: 1\n",
            "scales: 2\n",
            "dividing: 1\n",
            "pop: 1\n",
            "ulation: 1\n",
            "administering: 1\n",
            "percentiles: 1\n",
            "comparisons: 2\n",
            "transformed: 1\n",
            "static: 3\n",
            "stati: 1\n",
            "compilation: 1\n",
            "unjustified: 1\n",
            "inappropri: 1\n",
            "ate: 1\n",
            "wanted: 1\n",
            "removes: 1\n",
            "eat: 1\n",
            "005: 1\n",
            "001: 1\n",
            "hether: 1\n",
            "ob: 1\n",
            "served: 1\n",
            "probably: 1\n",
            "stratification: 1\n",
            "subgroups: 2\n",
            "homogenous: 1\n",
            "improve: 1\n",
            "stratified: 2\n",
            "theorized: 1\n",
            "structural: 1\n",
            "underlie: 1\n",
            "determines: 1\n",
            "precisely: 1\n",
            "subcultures: 1\n",
            "ethnic: 1\n",
            "regional: 1\n",
            "exhibiting: 1\n",
            "distingu: 1\n",
            "ish: 1\n",
            "belong: 1\n",
            "subjectivity: 1\n",
            "reflection: 1\n",
            "person¿s: 1\n",
            "mind: 1\n",
            "moods: 1\n",
            "limited: 1\n",
            "broken: 1\n",
            "subsamples: 1\n",
            "ou: 1\n",
            "resp: 1\n",
            "ondents: 1\n",
            "survival: 2\n",
            "prognosis: 1\n",
            "synchronic: 1\n",
            "medical: 1\n",
            "appraisal: 1\n",
            "appropria: 1\n",
            "combine: 1\n",
            "20th: 1\n",
            "30: 2\n",
            "z: 2\n",
            "multiplying: 1\n",
            "umbers: 1\n",
            "sheet: 1\n",
            "ideally: 1\n",
            "processing: 1\n",
            "knowl: 1\n",
            "edge: 1\n",
            "improved: 1\n",
            "testretest: 2\n",
            "rom: 1\n",
            "correlating: 1\n",
            "administrations: 2\n",
            "produces: 1\n",
            "theme: 2\n",
            "organizing: 1\n",
            "interpreting: 1\n",
            "progresses: 3\n",
            "recurring: 1\n",
            "emerges: 1\n",
            "conceptual: 3\n",
            "underpinning: 1\n",
            "observer’s: 1\n",
            "found: 1\n",
            "emerging: 2\n",
            "issues: 2\n",
            "repres: 1\n",
            "ented: 1\n",
            "organize: 1\n",
            "thick: 2\n",
            "rich: 1\n",
            "extens: 1\n",
            "ive: 1\n",
            "details: 1\n",
            "provision: 1\n",
            "datainformation: 1\n",
            "space: 1\n",
            "andmotion: 1\n",
            "logs: 1\n",
            "repeatedly: 2\n",
            "transferabi: 1\n",
            "lity: 1\n",
            "applicability: 1\n",
            "‘fit’: 1\n",
            "contexts: 1\n",
            "immediate: 1\n",
            "transfer: 1\n",
            "elsewhere: 1\n",
            "translation: 1\n",
            "decides: 1\n",
            "ss: 1\n",
            "translated: 1\n",
            "stimulus: 1\n",
            "ariable: 1\n",
            "trend: 2\n",
            "literally: 1\n",
            "repetitions: 1\n",
            "des: 2\n",
            "ign: 2\n",
            "diverse: 1\n",
            "pertaining: 1\n",
            "enhance: 1\n",
            "overly: 1\n",
            "rely: 1\n",
            "trustworthiness: 1\n",
            "reader: 1\n",
            "ttest: 1\n",
            "twostage: 1\n",
            "twotailed: 1\n",
            "unsure: 1\n",
            "twoway: 1\n",
            "anovas: 1\n",
            "concludes: 1\n",
            "decided: 1\n",
            "failure: 2\n",
            "really: 2\n",
            "alse: 1\n",
            "judged: 1\n",
            "typology: 2\n",
            "foundation: 1\n",
            "comprising: 1\n",
            "¿: 3\n",
            "overlap: 1\n",
            "exhaustive: 1\n",
            "unbalanced: 1\n",
            "favorable: 1\n",
            "unfavorable: 1\n",
            "free: 2\n",
            "serious: 1\n",
            "handled: 1\n",
            "resea: 1\n",
            "rchers: 1\n",
            "orientation: 2\n",
            "adopted: 1\n",
            "remember: 1\n",
            "deserves: 1\n",
            "atten: 1\n",
            "entity: 1\n",
            "univariate: 2\n",
            "standar: 1\n",
            "aware: 1\n",
            "inanimate: 1\n",
            "suspension: 1\n",
            "desired: 1\n",
            "inform: 1\n",
            "latitude: 1\n",
            "talk: 1\n",
            "bout: 1\n",
            "reflects: 1\n",
            "attempting: 1\n",
            "asuring: 1\n",
            "thing: 1\n",
            "you: 1\n",
            "squaring: 1\n",
            "duress: 1\n",
            "undue: 1\n",
            "inducement: 1\n",
            "subject’s: 1\n",
            "continue: 1\n",
            "weighted: 2\n",
            "sc: 1\n",
            "ore: 1\n",
            "weighting: 1\n",
            "weights: 1\n",
            "white: 2\n",
            "authoritative: 1\n",
            "architecture: 1\n",
            "produc: 1\n",
            "technology: 1\n",
            "business: 1\n",
            "facing: 1\n",
            "creswell: 2\n",
            "j: 4\n",
            "2017: 1\n",
            "sage: 2\n",
            "publications: 2\n",
            "fraenkel: 1\n",
            "wallen: 1\n",
            "hyun: 1\n",
            "1993: 1\n",
            "vol: 1\n",
            "7: 1\n",
            "york: 1\n",
            "mcgraw: 1\n",
            "hill: 1\n",
            "mackey: 1\n",
            "gass: 1\n",
            "2005: 1\n",
            "mahwah: 1\n",
            "nj: 1\n",
            "lawrence: 1\n",
            "erlbaum: 1\n",
            "associates: 1\n",
            "nunan: 1\n",
            "1992: 1\n",
            "cambridge: 2\n",
            "press: 1\n",
            "richards: 1\n",
            "morse: 1\n",
            "2007: 1\n",
            "readme: 1\n",
            "user’s: 1\n",
            "guid: 1\n",
            "2nd: 1\n",
            "thousand: 1\n",
            "oaks: 1\n",
            "ca: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def perform_stemming(text_path, stemmed_text_path):\n",
        "        # Read the text from the file\n",
        "        with open(text_path, 'r', encoding='utf-8') as text_file:\n",
        "            text = text_file.read()\n",
        "\n",
        "        # Tokenize the text\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        # Initialize the Porter stemmer\n",
        "        porter_stemmer = PorterStemmer()\n",
        "\n",
        "        # Perform stemming on each word\n",
        "        stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "        # Join the stemmed words into a single string\n",
        "        stemmed_text = ' '.join(stemmed_words)\n",
        "\n",
        "        # Save the stemmed text to a new file\n",
        "        with open(stemmed_text_path, 'w', encoding='utf-8') as stemmed_file:\n",
        "            stemmed_file.write(stemmed_text)\n",
        "\n",
        "        print(f\"Stemming successful. Stemmed text saved to: {stemmed_text_path}\")\n",
        "\n",
        "stemmed_text_file_path = '/content/drive/MyDrive/output_stemmed.txt'\n",
        "perform_stemming(text_file_path, stemmed_text_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTbFFIvOEtJO",
        "outputId": "0f1516df-3dad-46b5-a931-e3a92218bcf9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming successful. Stemmed text saved to: /content/drive/MyDrive/output_stemmed.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_stemmed.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "n5wLB3N2AZ0j",
        "outputId": "2b340d0f-64a3-4842-e66a-c45ada06a71d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-78ef26c83046>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_stemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output_stemmed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_frequencies(input_file_path, output_file_path):\n",
        "        # Read text from input file\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "            input_text = input_file.read()\n",
        "\n",
        "        # Read stemmed text from output file\n",
        "        with open(output_file_path, 'r', encoding='utf-8') as output_file:\n",
        "            output_text = output_file.read()\n",
        "\n",
        "        # Tokenize text\n",
        "        input_words = word_tokenize(input_text)\n",
        "        output_words = word_tokenize(output_text)\n",
        "\n",
        "        # Calculate word frequencies\n",
        "        input_freq = Counter(input_words)\n",
        "        output_freq = Counter(output_words)\n",
        "\n",
        "        # Compare frequencies\n",
        "        common_words = set(input_freq.keys()) & set(output_freq.keys())\n",
        "\n",
        "        print(\"Word frequencies comparison:\")\n",
        "        print(f\"{'Word': <15} {'Input Frequency': <15} {'Output Frequency': <15}\")\n",
        "        for word in common_words:\n",
        "            print(f\"{word: <15} {input_freq[word]: <15} {output_freq[word]: <15}\")\n",
        "\n",
        "compare_frequencies(pdf_file_path, stemmed_text_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "99T1O9iOHRbr",
        "outputId": "0a4a2dc5-43a2-4322-f7db-46c7189182a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e614c5b2360f>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{word: <15} {input_freq[word]: <15} {output_freq[word]: <15}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mcompare_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmed_text_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-e614c5b2360f>\u001b[0m in \u001b[0;36mcompare_frequencies\u001b[0;34m(input_file_path, output_file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;31m# Read text from input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Read stemmed text from output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb5 in position 11: invalid start byte"
          ]
        }
      ]
    }
  ]
}